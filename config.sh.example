#!/bin/bash
# ML Training Server - Configuration Template
# Copy this file to config.sh and customize for your setup:
#   cp config.sh.example config.sh
#   nano config.sh
#
# After editing, validate with: ./scripts/00-validate-config.sh
#
# NOTE: Storage configuration cannot be changed after running 01-setup-storage.sh
#       without wiping disks! All other settings can be changed and scripts re-run.

#==============================================================================
# SYSTEM CONFIGURATION
#==============================================================================

SERVER_HOSTNAME="ml-train-server"
MOUNT_POINT="/mnt/storage"

#==============================================================================
# USER CONFIGURATION
#==============================================================================

# List of users to create (space-separated)
# Example: "alice bob charlie dave eve"
# Example: "user1 user2 user3"
USERS="alice bob charlie dave eve"

# Starting UID for users (default: 1000)
FIRST_UID=1000

USER_SHELL="/bin/bash"
USER_GROUPS="docker sudo"

# Default password for all users (IMPORTANT: Change this!)
# Used for SSH, VNC, RDP, web desktop, and container login
USER_DEFAULT_PASSWORD="changeme_default_password"


#==============================================================================
# STORAGE CONFIGURATION
#==============================================================================

# Leave empty for auto-detection, or specify devices
# Auto-detection will find NVMe/SSD and all rotational HDDs

# SSD/NVMe device (auto-detected if empty)
# Example: "/dev/nvme0n1" or "/dev/sda"
NVME_DEVICE=""

# Size to reserve for OS (in GB), rest used for bcache
# Example: 50 (smaller OS partition), 200 (larger system partition)
OS_PARTITION_SIZE_GB=100

# HDD devices for BTRFS (auto-detected if empty)
# Example: "/dev/sdb /dev/sdc /dev/sdd /dev/sde"
HDD_DEVICES=""

# BTRFS RAID level - Choose based on disk count:
# - raid10: 4+ disks (best performance + redundancy, ~50% usable space)
# - raid1:  2+ disks (mirroring, ~50% usable space)
# - raid0:  2+ disks (fast but NO redundancy, ~100% usable space)
# - single: 1+ disk  (no redundancy, 100% usable space)
BTRFS_RAID_LEVEL="raid10"

# Compression: zstd:1-15 (higher=better compression, more CPU), lzo, zlib, none
# - zstd:1  = light compression (less CPU, less space savings)
# - zstd:3  = balanced (default)
# - zstd:15 = maximum compression (more CPU, more space savings)
BTRFS_COMPRESSION="zstd:3"

# bcache mode:
# - writeback:    Best performance, requires UPS
# - writethrough: Safer, slightly slower
# - writearound:  Write-around (rarely used)
# - none:         Disable bcache (if no SSD/NVMe)
BCACHE_MODE="writeback"

#==============================================================================
# RESOURCE LIMITS (PER USER)
#==============================================================================

# Minimum RAM guaranteed to each user container (Docker memory reservation)
MEMORY_GUARANTEE_GB=32

# Maximum RAM each user container can use (Docker memory limit)
# Container will be OOM-killed if it exceeds this
MEMORY_LIMIT_GB=100

# Swap space per user (helps prevent OOM kills during memory spikes)
SWAP_SIZE_GB=50

#==============================================================================
# DOCKER CONFIGURATION
#==============================================================================

# Docker storage driver (btrfs for BTRFS filesystem, overlay2 for ext4/xfs)
DOCKER_STORAGE_DRIVER="btrfs"

# Maximum size of each container log file before rotation
DOCKER_LOG_MAX_SIZE="10m"

# Number of rotated log files to keep per container
DOCKER_LOG_MAX_FILES="3"

# Require GPU to be available for containers to start
# Set to 'true' to fail container startup if the GPU is missing or inaccessible
# Set to 'false' to allow CPU-only operation with a warning
REQUIRE_GPU=true

#==============================================================================
# NETWORKING
#==============================================================================

# Your domain for Cloudflare Tunnel
# Example: "example.com" creates alice-code.example.com, alice-jupyter.example.com, etc.
DOMAIN=""

CODE_SERVER_PREFIX="code"
JUPYTER_PREFIX="jupyter"

# Docker bridge network subnet (must not conflict with your LAN/VPN)
# Default: 172.28.0.0/16 (chosen to avoid common conflicts)
# Common ranges to avoid:
#   - 172.16.0.0/12 (AWS VPCs, corporate networks)
#   - 172.20.0.0/16 (Cisco VPN, frequent Docker default)
#   - 192.168.0.0/16 (home networks)
#   - 10.0.0.0/8 (large corporate networks)
DOCKER_SUBNET="172.28.0.0/16"

# Local network CIDR for firewall (optional)
# Example: "192.168.1.0/24"
LOCAL_NETWORK_CIDR=""

#==============================================================================
# BACKUP CONFIGURATION
#==============================================================================

# Restic backup destination (rclone remote:path format)
# Example: "gdrive:backups/ml-train-server" or "s3:my-bucket/backups"
# Run 'rclone config' to create remotes first
BACKUP_REMOTE="gdrive:backups/ml-train-server"

# Restic repository password (set to a long, random string!)
# This value is written to /root/.restic-password and used for all restic operations.
RESTIC_PASSWORD="changeme_restic_password"

# Upload bandwidth limit for backups
BACKUP_BANDWIDTH_LIMIT_MBPS=100

# BTRFS snapshot retention policy
# Set to 0 to disable that tier (e.g., SNAPSHOT_KEEP_DAILY=0 for no daily snapshots)
SNAPSHOT_KEEP_HOURLY=24
SNAPSHOT_KEEP_DAILY=7
SNAPSHOT_KEEP_WEEKLY=4
SNAPSHOT_KEEP_MONTHLY=12
SNAPSHOT_KEEP_YEARLY=3

# Restic backup retention policy (how many backups to keep)
RESTIC_KEEP_DAILY=7
RESTIC_KEEP_WEEKLY=52

# Daily backup schedule (24-hour format)
BACKUP_HOUR=6
BACKUP_MINUTE=0

#==============================================================================
# DATA PIPELINE (Optional - for GCS to GDrive migration)
#==============================================================================

# Source: Google Cloud Storage bucket (rclone remote:bucket format)
# Leave empty to skip GCSâ†’GDrive sync. Run 'rclone config' to create remotes.
GCS_BUCKET="gcs:customer-daily-bucket"

# Destination: Google Drive folder (rclone remote:path format)
GDRIVE_CUSTOMER_DATA="gdrive:customer-daily"

# Download/upload bandwidth limit for data sync
DATA_SYNC_BANDWIDTH_LIMIT_MBPS=100

# Daily sync schedule (24-hour format)
DATA_SYNC_HOUR=4
DATA_SYNC_MINUTE=0

# Delete files from GCS after this many days (0 = never delete)
DATA_CLEANUP_DAYS=90

#==============================================================================
# STORAGE ALLOCATION LIMITS (per user)
#==============================================================================

# Per-user total storage quota (in GB)
# Covers combined usage across home directory, workspace, and docker volumes
# Breakdown example (not enforced separately):
#   - Home directory (~100GB): Code, configs, dotfiles, papers, venvs (backed up)
#   - Workspace (~800GB): Training data, model checkpoints, datasets (NOT backed up)
#   - Docker volumes (~100GB): Container persistent state (backed up)
# Total: 1TB per user
USER_QUOTA_GB=1000

# Quota warning threshold (percentage)
# Alert when user exceeds this percentage of their quota
USER_QUOTA_WARNING_PERCENT=80  # Warn at 80% usage (800GB for 1000GB quota)

# Storage safety margin (percentage)
# This percentage of free space (after user data + snapshots) is kept as buffer
# Prevents BTRFS performance degradation (performs poorly when >90% full)
# Also allows for unexpected growth and ensures VFS cache has eviction headroom
STORAGE_SAFETY_MARGIN_PERCENT=20  # Keep 20% free, VFS cache uses remaining 80%

# Estimated total storage capacity (in GB)
# Used for validation checks before storage setup
# Examples: 20000 (20TB for 2x20TB RAID1), 40000 (40TB for 4x20TB RAID10)
ESTIMATED_CAPACITY_GB=40000

#==============================================================================
# GOOGLE DRIVE SHARED DRIVE (for /shared mount)
#==============================================================================

# Remote name for Google Workspace Shared Drive
# Mounted at /shared with read-write access for all users to share files
# Create this remote during setup with: rclone config
GDRIVE_SHARED_REMOTE="gdrive-shared"

# Local cache directory for Google Drive files
# Large cache provides near-local performance
GDRIVE_CACHE_DIR="${MOUNT_POINT}/cache/gdrive"

# Maximum age for cached files (examples: "24h", "72h", "168h"/1week, "720h"/30days)
# Files not accessed for this duration will be evicted from cache
GDRIVE_CACHE_MAX_AGE="720h"

# rclone VFS cache mode (recommended: full)
# - full: Download entire file on first access, keep in cache
# - writes: Cache writes only
# - minimal: Minimal caching
GDRIVE_VFS_CACHE_MODE="full"

# Write-back cache duration (how long to buffer writes before uploading)
# Examples: "5s", "30s", "1m", "5m"
GDRIVE_WRITE_BACK="5s"

# Read chunk sizes (tune for performance)
# Larger chunks = fewer API calls but more memory
GDRIVE_READ_CHUNK_SIZE="128M"
GDRIVE_READ_CHUNK_LIMIT="off"  # Disable limit for best performance

# Buffer size for reading files
GDRIVE_BUFFER_SIZE="128M"

# Directory cache time (how long to cache directory listings)
GDRIVE_DIR_CACHE_TIME="5m"

# Poll interval for detecting remote changes
# Lower = more API calls but faster change detection
GDRIVE_POLL_INTERVAL="1m"

# Upload chunk size (must be multiple of 256K)
GDRIVE_CHUNK_SIZE="256M"

# Files smaller than this are uploaded in one request
GDRIVE_UPLOAD_CUTOFF="256M"

# Number of parallel transfers and checkers (increased for better performance)
GDRIVE_TRANSFERS=16  # Increased from 8 to 16 for faster sync
GDRIVE_CHECKERS=16

# Logging
GDRIVE_LOG_LEVEL="INFO"  # DEBUG, INFO, NOTICE, ERROR

#==============================================================================
# MONITORING & ALERTING
#==============================================================================

# Telegram Bot Configuration (leave empty to skip)
# How to set up:
# 1. Open Telegram and search for @BotFather
# 2. Send /newbot and follow the prompts to create your bot
# 3. Copy the bot token (looks like: 123456789:ABCdefGHIjklMNOpqrsTUVwxyz)
# 4. Start a chat with your new bot (send any message)
# 5. Message @userinfobot or @RawDataBot to get your chat ID
# 6. Add both values below
TELEGRAM_BOT_TOKEN=""
TELEGRAM_CHAT_ID=""

# healthchecks.io URLs for dead man's switch monitoring (leave empty to skip)
# Create checks at https://healthchecks.io and paste URLs here
HEALTHCHECK_BACKUP_URL=""
HEALTHCHECK_DATA_SYNC_URL=""

# Grafana admin password (IMPORTANT: Change this!)
# Used for Grafana web interface login (username: admin)
GRAFANA_ADMIN_PASSWORD="changeme_secure_password"

# Guacamole database password (IMPORTANT: Change this!)
GUACAMOLE_DB_PASSWORD="changeme_guacamole_password"

# Alert thresholds
GPU_TEMP_THRESHOLD=80          # GPU temperature in Celsius
FS_USAGE_THRESHOLD=90          # Filesystem usage percentage
SMART_MONITORING=true          # Enable SMART disk health monitoring

#==============================================================================
# SERVICE PORTS
#==============================================================================
# Note: VS Code and Jupyter are accessed via Traefik hostnames, not direct ports
# They run on container ports 8080 (code-server) and 8888 (jupyter) internally
# Access via: https://{username}-code.{domain} and https://{username}-jupyter.{domain}

TRAEFIK_PORT=8080
NETDATA_PORT=19999
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000
PORTAINER_PORT=9000
FILEBROWSER_PORT=8081
DOZZLE_PORT=8082
TENSORBOARD_PORT=6006
# SSH base port (increments per user: 2222, 2223, 2224, etc.)
SSH_BASE_PORT=2222

# VNC base port (increments per user: 5900, 5901, 5902, etc.)
VNC_BASE_PORT=5900

# RDP/XRDP base port (increments per user: 3389, 3390, 3391, etc.)
RDP_BASE_PORT=3389

# noVNC HTML5 base port (increments per user: 6080, 6081, 6082, etc.)
NOVNC_BASE_PORT=6080

# Guacamole port (single instance for all users)
GUACAMOLE_PORT=8090

# Kasm Workspaces port (single instance for all users)
KASM_PORT=443

#==============================================================================
# ADVANCED SETTINGS
#==============================================================================

# Enable auditd for security logging (logs all system calls)
ENABLE_AUDITD=false

# Enable UPS monitoring via apcupsd or nut (requires UPS hardware)
ENABLE_UPS_MONITORING=false

# Enable NVIDIA GPU timeslicing (share single GPU across multiple containers)
ENABLE_GPU_TIMESLICING=true

# CUDA version override (leave empty for auto-detection)
# Example: CUDA_VERSION="12.4.1"
CUDA_VERSION=""

# Docker Compose project name (prefix for all containers)
COMPOSE_PROJECT_NAME="ml-train-server"

#==============================================================================
# HELPER FUNCTIONS (DO NOT EDIT)
#==============================================================================

get_users() { echo ${USERS}; }
get_user_count() { echo ${USERS} | wc -w; }

get_user_uid() {
    local username=$1
    local index=0
    for user in ${USERS}; do
        [[ "$user" == "$username" ]] && echo $((FIRST_UID + index)) && return
        ((index++))
    done
}

get_user_port() {
    local username=$1
    local base_port=$2
    local index=0
    for user in ${USERS}; do
        [[ "$user" == "$username" ]] && echo $((base_port + index)) && return
        ((index++))
    done
}

detect_nvme_device() {
    [[ -n "${NVME_DEVICE}" ]] && echo "${NVME_DEVICE}" && return
    [[ -b "/dev/nvme0n1" ]] && echo "/dev/nvme0n1" && return
    [[ -b "/dev/sda" ]] && echo "/dev/sda" && return
}

detect_hdd_devices() {
    [[ -n "${HDD_DEVICES}" ]] && echo "${HDD_DEVICES}" && return
    local nvme_dev=$(detect_nvme_device)
    local hdds=""
    for dev in /dev/sd{b..z}; do
        if [[ -b "${dev}" ]] && [[ "${dev}" != "${nvme_dev}" ]]; then
            local disk_name=$(basename ${dev})
            if [[ -f "/sys/block/${disk_name}/queue/rotational" ]]; then
                [[ "$(cat /sys/block/${disk_name}/queue/rotational)" == "1" ]] && hdds="${hdds} ${dev}"
            fi
        fi
    done
    echo ${hdds}
}

get_rclone_bandwidth() { echo "${1}M"; }

# Export all
export SERVER_HOSTNAME MOUNT_POINT USERS FIRST_UID USER_SHELL USER_GROUPS USER_DEFAULT_PASSWORD
export NVME_DEVICE HDD_DEVICES OS_PARTITION_SIZE_GB
export BTRFS_RAID_LEVEL BTRFS_COMPRESSION BCACHE_MODE
export MEMORY_GUARANTEE_GB MEMORY_LIMIT_GB SWAP_SIZE_GB
export DOCKER_STORAGE_DRIVER DOCKER_LOG_MAX_SIZE DOCKER_LOG_MAX_FILES
export DOMAIN CODE_SERVER_PREFIX JUPYTER_PREFIX LOCAL_NETWORK_CIDR
export BACKUP_REMOTE BACKUP_BANDWIDTH_LIMIT_MBPS
export SNAPSHOT_KEEP_HOURLY SNAPSHOT_KEEP_DAILY SNAPSHOT_KEEP_WEEKLY SNAPSHOT_KEEP_MONTHLY SNAPSHOT_KEEP_YEARLY
export RESTIC_KEEP_DAILY RESTIC_KEEP_WEEKLY BACKUP_HOUR BACKUP_MINUTE
export GCS_BUCKET GDRIVE_CUSTOMER_DATA DATA_SYNC_BANDWIDTH_LIMIT_MBPS
export DATA_SYNC_HOUR DATA_SYNC_MINUTE DATA_CLEANUP_DAYS
export USER_QUOTA_GB USER_QUOTA_WARNING_PERCENT
export STORAGE_SAFETY_MARGIN_PERCENT ESTIMATED_CAPACITY_GB
export GDRIVE_SHARED_REMOTE GDRIVE_CACHE_DIR GDRIVE_CACHE_MAX_AGE
export GDRIVE_VFS_CACHE_MODE GDRIVE_WRITE_BACK GDRIVE_READ_CHUNK_SIZE GDRIVE_READ_CHUNK_LIMIT
export GDRIVE_BUFFER_SIZE GDRIVE_DIR_CACHE_TIME GDRIVE_POLL_INTERVAL
export GDRIVE_CHUNK_SIZE GDRIVE_UPLOAD_CUTOFF GDRIVE_TRANSFERS GDRIVE_CHECKERS GDRIVE_LOG_LEVEL
export TELEGRAM_BOT_TOKEN TELEGRAM_CHAT_ID HEALTHCHECK_BACKUP_URL HEALTHCHECK_DATA_SYNC_URL
export GRAFANA_ADMIN_PASSWORD GUACAMOLE_DB_PASSWORD GPU_TEMP_THRESHOLD FS_USAGE_THRESHOLD SMART_MONITORING
export TRAEFIK_PORT NETDATA_PORT PROMETHEUS_PORT GRAFANA_PORT PORTAINER_PORT
export FILEBROWSER_PORT DOZZLE_PORT TENSORBOARD_PORT SSH_BASE_PORT
export VNC_BASE_PORT RDP_BASE_PORT NOVNC_BASE_PORT GUACAMOLE_PORT KASM_PORT
export ENABLE_AUDITD ENABLE_UPS_MONITORING ENABLE_GPU_TIMESLICING CUDA_VERSION
export COMPOSE_PROJECT_NAME
